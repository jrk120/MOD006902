{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6cb01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, when, lag, round, sqrt, pow, sum as sum_func, max as max_func,row_number, lit, min as min_func, abs, avg, count\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, BooleanType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2597fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "spark = SparkSession.builder.appName(\"Formation insights\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8d8e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only desired columns and apply schema\n",
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"nflId\", DoubleType(), True),\n",
    "    StructField(\"frameId\", LongType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"frameType\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Merge all the tracking data into one dataframe.\n",
    "trackingDf = spark.read.schema(schema).parquet(\"data/tracking/\").cache()\n",
    "\n",
    "#Filter and select tracking data just before the snap - to determine if there is any pre-snap motion\n",
    "snapEventsDf = trackingDf.filter(col(\"frameType\") == \"SNAP\").groupby(\"gameId\", \"playId\").agg(min_func(\"frameId\").alias(\"snapEvent\"))\n",
    "\n",
    "#Join with full data to get last second before snap, using the last 10 frames (update rate of 0.1 seconds)\n",
    "presnapDf = trackingDf.join(snapEventsDf, [\"gameId\", \"playId\"]).filter(col(\"frameId\") < col(\"snapEvent\")).filter(col(\"frameId\") >= (col(\"snapEvent\") - 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa6d2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"yardlineNumber\", LongType(), True),\n",
    "    StructField(\"expectedPointsAdded\", DoubleType(), True),\n",
    "    StructField(\"offenseFormation\", StringType(), True), \n",
    "    StructField(\"receiverAlignment\", StringType(), True), \n",
    "    StructField(\"pff_passCoverage\", StringType(), True),\n",
    "    StructField(\"pff_manZone\", StringType(), True),\n",
    "    StructField(\"pff_runConceptPrimary\", StringType(), True)\n",
    "])\n",
    "\n",
    "playsDf = spark.read.schema(schema).parquet(\"data/plays.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfa1c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in columns of interest from players data source \n",
    "schema = StructType([\n",
    "    StructField(\"nflId\", LongType(), True), \n",
    "    StructField(\"position\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Read in players datasource and apply schema\n",
    "playersDf = spark.read.schema(schema).parquet(\"data/players.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "608c9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add positions column, through a join\n",
    "presnapDf = presnapDf.join(playersDf.select(\"nflId\", \"position\"), on=\"nflId\", how=\"left\")\n",
    "\n",
    "#Get rid of playersDf from cache as it obsolete\n",
    "playersDf.unpersist()\n",
    "\n",
    "#Position classifications for later usage\n",
    "offensivePositions = [\"QB\", \"RB\", \"FB\", \"HB\", \"WR\", \"TE\", \"LT\", \"LG\", \"C\", \"RG\", \"RT\"]\n",
    "defensivePositions = [\"CB\", \"S\", \"FS\", \"SS\", \"MLB\", \"OLB\", \"ILB\", \"LB\", \"DT\", \"DE\", \"NT\", \"DB\"]\n",
    "\n",
    "#Create a classifier column for if the player is on offence or defence\n",
    "presnapDf = presnapDf.withColumn(\"isOffence\",when(col(\"position\").isin(*offensivePositions), True).when(col(\"position\").isin(*defensivePositions), False).otherwise(lit(None)).cast(BooleanType()))\n",
    "\n",
    "#--------------------------------------------------Safety feature engineering---------------------------------------------------\n",
    "#Select only the safeties\n",
    "safetyDf = presnapDf.filter(col(\"position\").isin([\"SS\", \"FS\"]))\n",
    "\n",
    "#Drop redundant columns\n",
    "safetyDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"isOffence\")\n",
    "\n",
    "#Get the yardline of the play\n",
    "safetyDf = safetyDf.join(playsDf.select(\"gameId\", \"playId\", \"yardlineNumber\"), on = [\"gameId\", \"playId\"], how = \"left\")\n",
    "\n",
    "\n",
    "#Determine which direction the offence is facing\n",
    "safetyDf = safetyDf.withColumn(\"play_direction\", when(col(\"x\") < col(\"yardlineNumber\"), \"right\").otherwise(\"left\"))\n",
    "\n",
    "#Fix in the line of scrimmage, using the play direction, this is so that \"line_of_scrimmage\" is in the same units as \"x\"\n",
    "safetyDf = safetyDf.withColumn(\"line_of_scrimage\",when(col(\"play_direction\") == \"left\", 100 - col(\"yardlineNumber\")).otherwise(col(\"yardlineNumber\")))\n",
    "\n",
    "#Determine safety depth\n",
    "safetyDf = safetyDf.withColumn(\"distance\", abs(col(\"x\") - col(\"line_of_scrimage\")))\n",
    "\n",
    "#Select only the last frame, this will have the distance a moment before the snap\n",
    "frameGroupByDf = safetyDf.groupBy(\"nflId\", \"gameId\", \"playId\").agg(max_func(\"frameId\").alias(\"frameId\"))\n",
    "\n",
    "#what?\n",
    "safetyDf = safetyDf.join(frameGroupByDf, on = [\"nflId\",\"gameId\", \"playId\", \"frameId\"], how = \"inner\")\n",
    "\n",
    "#Create the two features by averaging the safety depth and creating a count of safeties\n",
    "safetyDf = safetyDf.groupBy(\"gameId\", \"playId\").agg(avg(\"distance\").alias(\"avgSafetyDistance\"), count(\"distance\").alias(\"numSafeties\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b5f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------Pre snap motion engineering---------------------------------------------------\n",
    "\n",
    "#Drop redundant columns\n",
    "presnapDf = presnapDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"position\")\n",
    "\n",
    "#Formalise the sequence of the dataframe through an id to be used in a partition windw\n",
    "presnapDf = presnapDf.withColumn(\"sequenceId\", monotonically_increasing_id())\n",
    "sparkWindow = Window.partitionBy(\"gameId\", \"playId\", \"nflId\").orderBy(\"sequenceId\")\n",
    "\n",
    "#Over the window (per player per play) determine their position compared to the last data point\n",
    "presnapDf = presnapDf.withColumn(\"prevX\", round(lag(\"x\").over(sparkWindow), 2))\n",
    "presnapDf = presnapDf.withColumn(\"prevY\", round(lag(\"y\").over(sparkWindow), 2))\n",
    "\n",
    "#Calculate cumulative distance\n",
    "presnapDf = presnapDf.withColumn(\"cumulativeDistance\", round(when(col(\"prevX\").isNull() | col(\"prevY\").isNull(), 0.0).otherwise(sqrt(pow(col(\"x\") - col(\"prevX\"), 2) + pow(col(\"y\") - col(\"prevY\"), 2))),5))\n",
    "\n",
    "#Sum all the movement values - this gives total distance moved in a second\n",
    "playerMotion = presnapDf.groupBy(\"gameId\", \"playId\", \"nflId\", \"isOffence\").agg(sum_func(\"cumulativeDistance\").alias(\"distanceMoved\"))\n",
    "\n",
    "presnapDf.unpersist()\n",
    "\n",
    "#Using a threshold of 2 yards in the 1 second time frame, determine if the player was in motion\n",
    "playerMotion = playerMotion.withColumn(\"motion\", col(\"distanceMoved\") > 2)\n",
    "\n",
    "#Determine if any player on each side was in motion\n",
    "playMotion = playerMotion.groupBy(\"gameId\", \"playId\", \"isOffence\").agg(max_func(\"motion\").alias(\"isMotion\"))\n",
    "\n",
    "#Create final result\n",
    "playMotion = playMotion.select(\"gameId\", \"playId\", \"isOffence\", \"isMotion\")\n",
    "\n",
    "#Move each play onto one row for an easier join with the rest of the data\n",
    "playMotion = playMotion.withColumn(\"isOffenceMoving\", when(col(\"isOffence\") == True, col(\"isMotion\")))\\\n",
    "                   .withColumn(\"isDefenceMoving\", when(col(\"isOffence\") == False, col(\"isMotion\")))\\\n",
    "                   .groupBy(\"gameId\", \"playId\")\\\n",
    "                   .agg(max_func(\"isOffenceMoving\").alias(\"isOffenceMoving\"),\n",
    "                        max_func(\"isDefenceMoving\").alias(\"isDefenceMoving\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d19e507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, playId: bigint, nflId: double, frameId: bigint, x: double, y: double, frameType: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entire Dataframe ready for ML workflow, join all feature tables together\n",
    "mergedDf = playsDf.join(playMotion, [\"gameId\", \"playId\"], \"inner\").join(safetyDf, [\"gameId\", \"playId\"], \"inner\")\n",
    "playMotion.unpersist()\n",
    "trackingDf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29fd8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6208063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing evaluation on LinearRegression !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 15:19:43 WARN Instrumentation: [8a6bd787] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/06/11 15:20:01 WARN Instrumentation: [8a6bd787] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression = 1.4334910939355152\n",
      "Commencing evaluation on GBT Regressor !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Regressor = 1.4405979896460048\n",
      "Commencing evaluation on Random Forest !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2367:==================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest = 1.4302780286061614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#Import pipeline features\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "#Import evaluator object\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#Import potential models\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "\n",
    "#Split the data \n",
    "trainDf, testDf = mergedDf.randomSplit([0.8, 0.2], seed = 1)\n",
    "\n",
    "target = \"expectedPointsAdded\"\n",
    "numFeatures = [\"gameId\", \"playId\", \"yardlineNumber\", \"avgSafetyDistance\", \"numSafeties\"]\n",
    "boolFeatures = [\"isOffenceMoving\", \"isDefenceMoving\"]\n",
    "textFeatures = [\"offenseFormation\", \"receiverAlignment\", \"pff_passCoverage\", \"pff_manZone\", \"pff_runConceptPrimary\"]\n",
    "\n",
    "#Text handler, pySpark's onehotencoder only accepts numerical input (hence StringIndexer)\n",
    "#Indexer maps strings to integers\n",
    "indexer = [StringIndexer(inputCol=columnName, outputCol=columnName + \"idx\", handleInvalid=\"keep\") for columnName in textFeatures]\n",
    "#Encoder maps integer to vector\n",
    "encoder = [OneHotEncoder(inputCol=columnName + \"idx\", outputCol=columnName + \"vector\") for columnName in textFeatures]\n",
    "\n",
    "assembler = VectorAssembler(inputCols= numFeatures + boolFeatures + [columnName + \"vector\" for columnName in textFeatures], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol= \"features\", outputCol=\"features-Scaled\")\n",
    "\n",
    "potentialModels = {\"LinearRegression\": LinearRegression(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"GBT Regressor\": GBTRegressor(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"Random Forest\" : RandomForestRegressor(featuresCol=\"features-Scaled\", labelCol=target)\n",
    "                   }\n",
    "\n",
    "evalObj = RegressionEvaluator(metricName=\"rmse\", labelCol=target, predictionCol=\"prediction\")\n",
    "\n",
    "colsToDrop = numFeatures + boolFeatures + textFeatures + [columnName + \"idx\" for columnName in textFeatures] + [columnName + \"vector\" for columnName in textFeatures]\n",
    "\n",
    "\n",
    "for modelName, model in potentialModels.items():\n",
    "    print(f\"Commencing evaluation on {modelName} !\")\n",
    "\n",
    "    pipeline = Pipeline(stages = indexer + encoder + [assembler, scaler, model])\n",
    "\n",
    "    fullPipelineObj = pipeline.fit(trainDf)\n",
    "\n",
    "    predictions = fullPipelineObj.transform(testDf)\n",
    "    \n",
    "    predictions.drop(*colsToDrop)\n",
    "    \n",
    "    rmse = evalObj.evaluate(predictions)\n",
    "\n",
    "    print(f\"{modelName} = {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
