{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6cb01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, when, lag, round, sqrt, pow, sum as sum_func, max as max_func,row_number, lit, min as min_func, abs, avg, count\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, BooleanType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "spark = SparkSession.builder.appName(\"Formation insights\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8d8e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only desired columns and apply schema\n",
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"nflId\", DoubleType(), True),\n",
    "    StructField(\"frameId\", LongType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"frameType\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Merge all the tracking data into one dataframe.\n",
    "trackingDf = spark.read.schema(schema).parquet(\"data/tracking/\").cache()\n",
    "\n",
    "#Filter and select tracking data just before the snap - to determine if there is any pre-snap motion\n",
    "snapEventsDf = trackingDf.filter(col(\"frameType\") == \"SNAP\").groupby(\"gameId\", \"playId\").agg(min_func(\"frameId\").alias(\"snapEvent\"))\n",
    "\n",
    "#Join with full data to get last second before snap, using the last 10 frames (update rate of 0.1 seconds)\n",
    "presnapDf = trackingDf.join(snapEventsDf, [\"gameId\", \"playId\"]).filter(col(\"frameId\") < col(\"snapEvent\")).filter(col(\"frameId\") >= (col(\"snapEvent\") - 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa6d2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"yardlineNumber\", LongType(), True),\n",
    "    StructField(\"expectedPointsAdded\", DoubleType(), True),\n",
    "    StructField(\"offenseFormation\", StringType(), True), \n",
    "    StructField(\"receiverAlignment\", StringType(), True), \n",
    "    StructField(\"pff_passCoverage\", StringType(), True),\n",
    "    StructField(\"pff_manZone\", StringType(), True),\n",
    "    StructField(\"pff_runConceptPrimary\", StringType(), True)\n",
    "])\n",
    "\n",
    "playsDf = spark.read.schema(schema).parquet(\"data/plays.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa1c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in columns of interest from players data source \n",
    "schema = StructType([\n",
    "    StructField(\"nflId\", LongType(), True), \n",
    "    StructField(\"position\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Read in players datasource and apply schema\n",
    "playersDf = spark.read.schema(schema).parquet(\"data/players.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608c9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add positions column, through a join\n",
    "presnapDf = presnapDf.join(playersDf.select(\"nflId\", \"position\"), on=\"nflId\", how=\"left\")\n",
    "\n",
    "#Get rid of playersDf from cache as it obsolete\n",
    "playersDf.unpersist()\n",
    "\n",
    "#Position classifications for later usage\n",
    "offensivePositions = [\"QB\", \"RB\", \"FB\", \"HB\", \"WR\", \"TE\", \"LT\", \"LG\", \"C\", \"RG\", \"RT\"]\n",
    "defensivePositions = [\"CB\", \"S\", \"FS\", \"SS\", \"MLB\", \"OLB\", \"ILB\", \"LB\", \"DT\", \"DE\", \"NT\", \"DB\"]\n",
    "\n",
    "#Create a classifier column for if the player is on offence or defence\n",
    "presnapDf = presnapDf.withColumn(\"isOffence\",when(col(\"position\").isin(*offensivePositions), True).when(col(\"position\").isin(*defensivePositions), False).otherwise(lit(None)).cast(BooleanType()))\n",
    "\n",
    "#--------------------------------------------------Safety feature engineering---------------------------------------------------\n",
    "#Select only the safeties\n",
    "safetyDf = presnapDf.filter(col(\"position\").isin([\"SS\", \"FS\"]))\n",
    "\n",
    "#Drop redundant columns\n",
    "safetyDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"isOffence\")\n",
    "\n",
    "#Get the yardline of the play\n",
    "safetyDf = safetyDf.join(playsDf.select(\"gameId\", \"playId\", \"yardlineNumber\"), on = [\"gameId\", \"playId\"], how = \"left\")\n",
    "\n",
    "\n",
    "#Determine which direction the offence is facing\n",
    "safetyDf = safetyDf.withColumn(\"play_direction\", when(col(\"x\") < col(\"yardlineNumber\"), \"right\").otherwise(\"left\"))\n",
    "\n",
    "#Fix in the line of scrimmage, using the play direction, this is so that \"line_of_scrimmage\" is in the same units as \"x\"\n",
    "safetyDf = safetyDf.withColumn(\"line_of_scrimage\",when(col(\"play_direction\") == \"left\", 100 - col(\"yardlineNumber\")).otherwise(col(\"yardlineNumber\")))\n",
    "\n",
    "#Determine safety depth\n",
    "safetyDf = safetyDf.withColumn(\"distance\", abs(col(\"x\") - col(\"line_of_scrimage\")))\n",
    "\n",
    "#Select only the last frame, this will have the distance a moment before the snap\n",
    "frameGroupByDf = safetyDf.groupBy(\"nflId\", \"gameId\", \"playId\").agg(max_func(\"frameId\").alias(\"frameId\"))\n",
    "\n",
    "#what?\n",
    "safetyDf = safetyDf.join(frameGroupByDf, on = [\"nflId\",\"gameId\", \"playId\", \"frameId\"], how = \"inner\")\n",
    "\n",
    "#Create the two features by averaging the safety depth and creating a count of safeties\n",
    "safetyDf = safetyDf.groupBy(\"gameId\", \"playId\").agg(avg(\"distance\").alias(\"avgSafetyDistance\"), count(\"distance\").alias(\"numSafeties\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b5f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------Pre snap motion engineering---------------------------------------------------\n",
    "\n",
    "#Drop redundant columns\n",
    "presnapDf = presnapDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"position\")\n",
    "\n",
    "#Formalise the sequence of the dataframe through an id to be used in a partition windw\n",
    "presnapDf = presnapDf.withColumn(\"sequenceId\", monotonically_increasing_id())\n",
    "sparkWindow = Window.partitionBy(\"gameId\", \"playId\", \"nflId\").orderBy(\"sequenceId\")\n",
    "\n",
    "#Over the window (per player per play) determine their position compared to the last data point\n",
    "presnapDf = presnapDf.withColumn(\"prevX\", round(lag(\"x\").over(sparkWindow), 2))\n",
    "presnapDf = presnapDf.withColumn(\"prevY\", round(lag(\"y\").over(sparkWindow), 2))\n",
    "\n",
    "#Calculate cumulative distance\n",
    "presnapDf = presnapDf.withColumn(\"cumulativeDistance\", round(when(col(\"prevX\").isNull() | col(\"prevY\").isNull(), 0.0).otherwise(sqrt(pow(col(\"x\") - col(\"prevX\"), 2) + pow(col(\"y\") - col(\"prevY\"), 2))),5))\n",
    "\n",
    "#Sum all the movement values - this gives total distance moved in a second\n",
    "playerMotion = presnapDf.groupBy(\"gameId\", \"playId\", \"nflId\", \"isOffence\").agg(sum_func(\"cumulativeDistance\").alias(\"distanceMoved\"))\n",
    "\n",
    "presnapDf.unpersist()\n",
    "\n",
    "#Using a threshold of 2 yards in the 1 second time frame, determine if the player was in motion\n",
    "playerMotion = playerMotion.withColumn(\"motion\", col(\"distanceMoved\") > 2)\n",
    "\n",
    "#Determine if any player on each side was in motion\n",
    "playMotion = playerMotion.groupBy(\"gameId\", \"playId\", \"isOffence\").agg(max_func(\"motion\").alias(\"isMotion\"))\n",
    "\n",
    "#Create final result\n",
    "playMotion = playMotion.select(\"gameId\", \"playId\", \"isOffence\", \"isMotion\")\n",
    "\n",
    "#Move each play onto one row for an easier join with the rest of the data\n",
    "playMotion = playMotion.withColumn(\"isOffenceMoving\", when(col(\"isOffence\") == True, col(\"isMotion\")))\\\n",
    "                   .withColumn(\"isDefenceMoving\", when(col(\"isOffence\") == False, col(\"isMotion\")))\\\n",
    "                   .groupBy(\"gameId\", \"playId\")\\\n",
    "                   .agg(max_func(\"isOffenceMoving\").alias(\"isOffenceMoving\"),\n",
    "                        max_func(\"isDefenceMoving\").alias(\"isDefenceMoving\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d19e507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, playId: bigint, nflId: double, frameId: bigint, x: double, y: double, frameType: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entire Dataframe ready for ML workflow, join all feature tables together\n",
    "mergedDf = playsDf.join(playMotion, [\"gameId\", \"playId\"], \"inner\").join(safetyDf, [\"gameId\", \"playId\"], \"inner\")\n",
    "playMotion.unpersist()\n",
    "trackingDf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29fd8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6208063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#Import pipeline features\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "#Import evaluator object\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#Import potential models\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "\n",
    "#Split the data \n",
    "trainDf, testDf = mergedDf.randomSplit([0.8, 0.2], seed = 1)\n",
    "\n",
    "target = \"expectedPointsAdded\"\n",
    "numFeatures = [\"gameId\", \"playId\", \"yardlineNumber\", \"avgSafetyDistance\", \"numSafeties\"]\n",
    "boolFeatures = [\"isOffenceMoving\", \"isDefenceMoving\"]\n",
    "textFeatures = [\"offenseFormation\", \"receiverAlignment\", \"pff_passCoverage\", \"pff_manZone\", \"pff_runConceptPrimary\"]\n",
    "\n",
    "#Text handler, pySpark's onehotencoder only accepts numerical input (hence StringIndexer)\n",
    "#Indexer maps strings to integers\n",
    "indexer = [StringIndexer(inputCol=columnName, outputCol=columnName + \"idx\", handleInvalid=\"keep\") for columnName in textFeatures]\n",
    "#Encoder maps integer to vector\n",
    "encoder = [OneHotEncoder(inputCol=columnName + \"idx\", outputCol=columnName + \"vector\") for columnName in textFeatures]\n",
    "\n",
    "assembler = VectorAssembler(inputCols= numFeatures + boolFeatures + [columnName + \"vector\" for columnName in textFeatures], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol= \"features\", outputCol=\"features-Scaled\")\n",
    "\n",
    "potentialModels = {\"LinearRegression\": LinearRegression(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"GBT Regressor\": GBTRegressor(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"Random Forest\" : RandomForestRegressor(featuresCol=\"features-Scaled\", labelCol=target)\n",
    "                   }\n",
    "\n",
    "evalObj = RegressionEvaluator(metricName=\"rmse\", labelCol=target, predictionCol=\"prediction\")\n",
    "\n",
    "colsToDrop = numFeatures + boolFeatures + textFeatures + [columnName + \"idx\" for columnName in textFeatures] + [columnName + \"vector\" for columnName in textFeatures]\n",
    "\n",
    "\n",
    "#for modelName, model in potentialModels.items():\n",
    " #   print(f\"Commencing evaluation on {modelName} !\")\n",
    "#\n",
    " #   pipeline = Pipeline(stages = indexer + encoder + [assembler, scaler, model])\n",
    "#\n",
    " #   fullPipelineObj = pipeline.fit(trainDf)\n",
    "#\n",
    " #   predictions = fullPipelineObj.transform(testDf)\n",
    "  #  \n",
    "   # predictions.drop(*colsToDrop)\n",
    "    #\n",
    "    #rmse = evalObj.evaluate(predictions)\n",
    "\n",
    "    #print(f\"{modelName} = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0569c4",
   "metadata": {},
   "source": [
    "Commencing evaluation on LinearRegression !\n",
    "25/06/11 15:19:43 WARN Instrumentation: [8a6bd787] regParam is zero, which might cause numerical instability and overfitting.\n",
    "25/06/11 15:20:01 WARN Instrumentation: [8a6bd787] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
    "                                                                                \n",
    "LinearRegression = 1.4334910939355152\n",
    "Commencing evaluation on GBT Regressor !\n",
    "                                                                                \n",
    "GBT Regressor = 1.4405979896460048\n",
    "Commencing evaluation on Random Forest !\n",
    "[Stage 2367:==================================================>   (13 + 1) / 14]\n",
    "Random Forest = 1.4302780286061614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 918:======>      (102 + 2) / 200][Stage 920:>              (0 + 0) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/multiprocessing/pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      7\u001b[39m paraGrid = ParamGridBuilder() \\\n\u001b[32m      8\u001b[39m     .addGrid(RFR.numTrees, [\u001b[32m10\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m]) \\\n\u001b[32m      9\u001b[39m     .addGrid(RFR.maxDepth, [\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m]) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     .addGrid(RFR.minInstancesPerNode, [\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m]) \\\n\u001b[32m     13\u001b[39m     .build()\n\u001b[32m     15\u001b[39m CrossVal = CrossValidator(\n\u001b[32m     16\u001b[39m     estimator=pipeline,\n\u001b[32m     17\u001b[39m     estimatorParamMaps=paraGrid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     parallelism=\u001b[32m2\u001b[39m  \u001b[38;5;66;03m# increase this based on your Spark cluster\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m CrossValModel = \u001b[43mCrossVal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDf\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     25\u001b[39m predictions = CrossValModel.transform(testDf)\n\u001b[32m     26\u001b[39m rmse = evalObj.evaluate(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/pyspark/ml/tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    852\u001b[39m train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/multiprocessing/pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 918:=======>     (112 + 2) / 200][Stage 920:>              (0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "RFR = RandomForestRegressor(featuresCol=\"features-Scaled\", labelCol=target)\n",
    "\n",
    "pipeline = Pipeline(stages = indexer + encoder + [assembler, scaler, RFR])\n",
    "\n",
    "paraGrid = ParamGridBuilder() \\\n",
    "    .addGrid(RFR.numTrees, [20, 50, 100, 200]) \\\n",
    "    .addGrid(RFR.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(RFR.maxBins, [32, 64]) \\\n",
    "    .addGrid(RFR.minInstancesPerNode, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "CrossVal = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paraGrid,\n",
    "    evaluator=evalObj,\n",
    "    numFolds=3,\n",
    ")\n",
    "\n",
    "CrossValModel = CrossVal.fit(trainDf) \n",
    "\n",
    "predictions = CrossValModel.transform(testDf)\n",
    "rmse = evalObj.evaluate(predictions)\n",
    "print(f\"Optimised RFR: {rmse}\")\n",
    "\n",
    "trainDf.unpersist()\n",
    "del trainDf\n",
    "testDf.unpersist()\n",
    "del testDf\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "bestModel = CrossValModel.bestModel\n",
    "\n",
    "RFR_model = bestModel.stages[-1]\n",
    "\n",
    "bestPara = {\n",
    "    'numTrees': RFR_model.getNumTrees,\n",
    "    'maxDepth': RFR_model.getMaxDepth,\n",
    "    'maxBins': RFR_model.getMaxBins,\n",
    "    'minInstancesPerNode': RFR_model.getMinInstancesPerNode,\n",
    "    'featureSubsetStrategy': RFR_model.getFeatureSubsetStrategy\n",
    "}\n",
    "\n",
    "for hyperPara, value in bestPara.items():\n",
    "    print(f\"  {hyperPara}: {value()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
