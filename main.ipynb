{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cb01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, when, lag, round, sqrt, pow, sum as sum_func, max as max_func, lit, min as min_func, abs, avg, count\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, BooleanType, DoubleType, ByteType, ShortType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Steve\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only desired columns and apply schema\n",
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"nflId\", DoubleType(), True),\n",
    "    StructField(\"frameId\", LongType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"frameType\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Read all the tracking data (which should have same schema) into one dataframe\n",
    "folder = \"data/tracking/\"\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "trackingDf = spark.read.schema(schema).parquet(*files)\n",
    "\n",
    "#Filter and select tracking data just before the snap - to determine if there is any pre-snap motion\n",
    "snapEventsDf = trackingDf.filter(col(\"frameType\") == \"SNAP\").groupby(\"gameId\", \"playId\").agg(min_func(\"frameId\").alias(\"snapEvent\"))\n",
    "\n",
    "#Join with full data to get last second before snap, using the last 10 frames (update rate of 0.1 seconds)\n",
    "presnapDf = trackingDf.join(snapEventsDf, [\"gameId\", \"playId\"]).filter(col(\"frameId\") < col(\"snapEvent\")).filter(col(\"frameId\") >= (col(\"snapEvent\") - 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6d2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gameId\", LongType(), True),\n",
    "    StructField(\"playId\", LongType(), True),\n",
    "    StructField(\"yardlineNumber\", LongType(), True),\n",
    "    StructField(\"expectedPointsAdded\", DoubleType(), True),\n",
    "    StructField(\"offenseFormation\", StringType(), True), \n",
    "    StructField(\"receiverAlignment\", StringType(), True), \n",
    "    StructField(\"pff_passCoverage\", StringType(), True),\n",
    "    StructField(\"pff_manZone\", StringType(), True),\n",
    "    StructField(\"pff_runConceptPrimary\", StringType(), True)\n",
    "])\n",
    "\n",
    "playsDf = spark.read.schema(schema).parquet(\"data/plays.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b347c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#playsDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee363eb7",
   "metadata": {},
   "source": [
    "+-------+--------------------+------------------+------------------+--------------------+----------------+-----------------+----------------+-----------+---------------------+\n",
    "|summary|              gameId|            playId|    yardlineNumber| expectedPointsAdded|offenseFormation|receiverAlignment|pff_passCoverage|pff_manZone|pff_runConceptPrimary|\n",
    "+-------+--------------------+------------------+------------------+--------------------+----------------+-----------------+----------------+-----------+---------------------+\n",
    "|  count|               16124|             16124|             16124|               16124|           15936|            15936|           15932|      15932|                 9071|\n",
    "|   mean|2.0220989036912057E9|2023.8305631356984|29.226184569585712|-0.02938121470672...|            NULL|             NULL|            NULL|       NULL|                 NULL|\n",
    "| stddev|   5979.251755132589|1182.0907707451506|12.662827976390345|   1.397405113935573|            NULL|             NULL|            NULL|       NULL|                 NULL|\n",
    "|    min|          2022090800|                54|                 1|   -13.0236002178863|           EMPTY|              1x0|           2-Man|        Man|              COUNTER|\n",
    "|    max|          2022110700|              5120|                50|     8.6989859752357|         WILDCAT|              4x2|        Red Zone|       Zone|            UNDEFINED|\n",
    "+-------+--------------------+------------------+------------------+--------------------+----------------+-----------------+----------------+-----------+---------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4391dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#playsDf.select(\"expectedPointsAdded\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f677df",
   "metadata": {},
   "source": [
    "+-------+--------------------+\n",
    "|summary| expectedPointsAdded|\n",
    "+-------+--------------------+\n",
    "|  count|               16124|\n",
    "|   mean|-0.02938121470672...|\n",
    "| stddev|   1.397405113935573|\n",
    "|    min|   -13.0236002178863|\n",
    "|    max|     8.6989859752357|\n",
    "+-------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34519d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS JUST FOR ONE TIME VIEWING - not included in full workflow\n",
    "def analyse_numerical(df):\n",
    "    df = playsDf.toPandas()\n",
    "    df = df.drop(['gameId', 'playId'], axis=1)\n",
    "    df.hist(bins=20, figsize=(7, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04778ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS JUST FOR ONE TIME VIEWING - not included in full workflow\n",
    "def analyse_text(df):\n",
    "    textCols = df.select_dtypes(include = ['object', 'category']).columns\n",
    "\n",
    "    for columnName in textCols:\n",
    "        column = df[columnName].fillna('missing')\n",
    "        plt.figure(figsize = (10,10))\n",
    "        sns.countplot(data=df, x=columnName, order=df[columnName].value_counts().index[:15])\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f\"{columnName} Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154ffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS JUST FOR ONE TIME VIEWING - not included in full workflow\n",
    "def correlation(df):\n",
    "    encodedDf = pd.get_dummies(df, drop_first=True)\n",
    "    correlations = encodedDf.corr()\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    sns.heatmap(correlations, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix - One Hot Encoded')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa1c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in columns of interest from players data source \n",
    "schema = StructType([\n",
    "    StructField(\"nflId\", LongType(), True), \n",
    "    StructField(\"position\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Read in players datasource and apply schema\n",
    "playersDf = spark.read.schema(schema).parquet(\"data/players.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "608c9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add positions column, through a join\n",
    "presnapDf = presnapDf.join(playersDf.select(\"nflId\", \"position\"), on=\"nflId\", how=\"left\")\n",
    "\n",
    "#Get rid of playersDf from cache as it is obsolete\n",
    "playersDf.unpersist()\n",
    "\n",
    "#Position classifications for later usage\n",
    "offensivePositions = [\"QB\", \"RB\", \"FB\", \"HB\", \"WR\", \"TE\", \"LT\", \"LG\", \"C\", \"RG\", \"RT\"]\n",
    "defensivePositions = [\"CB\", \"S\", \"FS\", \"SS\", \"MLB\", \"OLB\", \"ILB\", \"LB\", \"DT\", \"DE\", \"NT\", \"DB\"]\n",
    "\n",
    "#Create a classifier column for if the player is on offence or defence\n",
    "presnapDf = presnapDf.withColumn(\"isOffence\",when(col(\"position\").isin(*offensivePositions), True).when(col(\"position\").isin(*defensivePositions), False).otherwise(lit(None)).cast(BooleanType()))\n",
    "\n",
    "#--------------------------------------------------Safety feature engineering---------------------------------------------------\n",
    "#Select only the safeties\n",
    "safetyDf = presnapDf.filter(col(\"position\").isin([\"SS\", \"FS\"]))\n",
    "\n",
    "safetyDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"isOffence\")\n",
    "\n",
    "#Get the yardline of the play\n",
    "safetyDf = safetyDf.join(playsDf.select(\"gameId\", \"playId\", \"yardlineNumber\"), on = [\"gameId\", \"playId\"], how = \"left\")\n",
    "\n",
    "#Determine which direction the offence is facing\n",
    "safetyDf = safetyDf.withColumn(\"play_direction\", when(col(\"x\") < col(\"yardlineNumber\"), \"right\").otherwise(\"left\"))\n",
    "\n",
    "#Fix in the line of scrimmage, using the play direction, this is so that \"line_of_scrimmage\" is in the same units as \"x\"\n",
    "safetyDf = safetyDf.withColumn(\"line_of_scrimage\",when(col(\"play_direction\") == \"left\", 100 - col(\"yardlineNumber\")).otherwise(col(\"yardlineNumber\")))\n",
    "\n",
    "#Determine safety depth\n",
    "safetyDf = safetyDf.withColumn(\"distance\", abs(col(\"x\") - col(\"line_of_scrimage\")))\n",
    "\n",
    "#Select only the last frame, this will have the distance a moment before the snap\n",
    "frameGroupByDf = safetyDf.groupBy(\"nflId\", \"gameId\", \"playId\").agg(max_func(\"frameId\").alias(\"frameId\"))\n",
    "\n",
    "#Add the new calculated columns\n",
    "safetyDf = safetyDf.join(frameGroupByDf, on = [\"nflId\",\"gameId\", \"playId\", \"frameId\"], how = \"inner\")\n",
    "\n",
    "#Create the two features by averaging the safety depth and creating a count of safeties, (the new calculated columns)\n",
    "safetyDf = safetyDf.groupBy(\"gameId\", \"playId\").agg(avg(\"distance\").alias(\"avgSafetyDistance\"), count(\"distance\").alias(\"numSafeties\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------Pre snap motion engineering---------------------------------------------------\n",
    "\n",
    "#Drop redundant columns\n",
    "presnapDf = presnapDf.drop(\"frameId\", \"frameType\", \"snapEvent\", \"position\")\n",
    "\n",
    "#Formalise the sequence of the dataframe through an id to be used in a partition windw\n",
    "presnapDf = presnapDf.withColumn(\"sequenceId\", monotonically_increasing_id())\n",
    "sparkWindow = Window.partitionBy(\"gameId\", \"playId\", \"nflId\").orderBy(\"sequenceId\")\n",
    "\n",
    "#Over the window (per player per play) determine their position compared to the last data point\n",
    "presnapDf = presnapDf.withColumn(\"prevX\", round(lag(\"x\").over(sparkWindow), 2))\n",
    "presnapDf = presnapDf.withColumn(\"prevY\", round(lag(\"y\").over(sparkWindow), 2))\n",
    "\n",
    "#Calculate cumulative distance\n",
    "presnapDf = presnapDf.withColumn(\"cumulativeDistance\", round(when(col(\"prevX\").isNull() | col(\"prevY\").isNull(), 0.0).otherwise(sqrt(pow(col(\"x\") - col(\"prevX\"), 2) + pow(col(\"y\") - col(\"prevY\"), 2))),5))\n",
    "\n",
    "#Sum all the movement values - this gives total distance moved in a second\n",
    "playerMotion = presnapDf.groupBy(\"gameId\", \"playId\", \"nflId\", \"isOffence\").agg(sum_func(\"cumulativeDistance\").alias(\"distanceMoved\"))\n",
    "\n",
    "presnapDf.unpersist()\n",
    "\n",
    "#Using a threshold of 2 yards in the 1 second time frame, determine if the player was in motion\n",
    "playerMotion = playerMotion.withColumn(\"motion\", col(\"distanceMoved\") > 2)\n",
    "\n",
    "#Determine if any player on each side was in motion\n",
    "playMotion = playerMotion.groupBy(\"gameId\", \"playId\", \"isOffence\").agg(max_func(\"motion\").alias(\"isMotion\"))\n",
    "\n",
    "#Create final result\n",
    "playMotion = playMotion.select(\"gameId\", \"playId\", \"isOffence\", \"isMotion\")\n",
    "\n",
    "#Move each play onto one row for an easier join with the rest of the data\n",
    "playMotion = playMotion.withColumn(\"isOffenceMoving\", when(col(\"isOffence\") == True, col(\"isMotion\")))\\\n",
    "                   .withColumn(\"isDefenceMoving\", when(col(\"isOffence\") == False, col(\"isMotion\")))\\\n",
    "                   .groupBy(\"gameId\", \"playId\")\\\n",
    "                   .agg(max_func(\"isOffenceMoving\").alias(\"isOffenceMoving\"),\n",
    "                        max_func(\"isDefenceMoving\").alias(\"isDefenceMoving\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19e507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, playId: bigint, nflId: double, frameId: bigint, x: double, y: double, frameType: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entire Dataframe ready for ML workflow, join all feature tables together\n",
    "mergedDf = playsDf.join(playMotion, [\"gameId\", \"playId\"], \"inner\").join(safetyDf, [\"gameId\", \"playId\"], \"inner\")\n",
    "mergedDf = mergedDf.withColumn(\"numSafeties\", mergedDf[\"numSafeties\"].cast(ByteType()))\n",
    "mergedDf = mergedDf.withColumn(\"yardlineNumber\", mergedDf[\"yardlineNumber\"].cast(ShortType()))\n",
    "mergedDf.cache()\n",
    "playMotion.unpersist()\n",
    "trackingDf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8af9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(mergedDf, fraction):\n",
    "    '''\n",
    "    This function is used to down sample the majoirty class when needed\n",
    "    inputs: \n",
    "        mergedDf (pySpark DataFrame) - This is the processed data ready to be split \n",
    "        fraction (float) - The fraction that the user wants to keep (0.8 = keeps 80%)\n",
    "    outputs:\n",
    "        mergedDf (pySpark DataFrame) - Now down sampled Dataframe\n",
    "    '''\n",
    "    #Grab all the data points within the majority range (-1 to 1)\n",
    "    majorityDf = mergedDf.filter((col(\"expectedPointsAdded\") >= -1) & (col(\"expectedPointsAdded\") <=1))\n",
    "\n",
    "    #Select a fraction of them\n",
    "    majorityDf = majorityDf.sample(fraction = fraction, seed = 1)\n",
    "\n",
    "    #Select everything that ISNT in the majority range\n",
    "    mergedDf = mergedDf.filter((col(\"expectedPointsAdded\") < -1) | (col(\"expectedPointsAdded\") > 1))\n",
    "\n",
    "    #Union join to bring the two together\n",
    "    mergedDf = majorityDf.union(mergedDf)\n",
    "\n",
    "    return mergedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29fd8396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gameId: bigint, playId: bigint, yardlineNumber: smallint, expectedPointsAdded: double, offenseFormation: string, receiverAlignment: string, pff_passCoverage: string, pff_manZone: string, pff_runConceptPrimary: string, isOffenceMoving: boolean, isDefenceMoving: boolean, avgSafetyDistance: double, numSafeties: tinyint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data \n",
    "trainDf, testDf = mergedDf.randomSplit([0.8, 0.2], seed = 1)\n",
    "trainDf.cache()\n",
    "testDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6208063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#Import pipeline features\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "#Import evaluator object\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#Import potential models\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "\n",
    "\n",
    "target = \"expectedPointsAdded\"\n",
    "numFeatures = [\"gameId\", \"playId\", \"yardlineNumber\", \"avgSafetyDistance\", \"numSafeties\"]\n",
    "boolFeatures = [\"isOffenceMoving\", \"isDefenceMoving\"]\n",
    "textFeatures = [\"offenseFormation\", \"receiverAlignment\", \"pff_passCoverage\", \"pff_manZone\", \"pff_runConceptPrimary\"]\n",
    "\n",
    "#Text handler, pySpark's onehotencoder only accepts numerical input (hence StringIndexer)\n",
    "#Indexer maps strings to integers\n",
    "indexer = [StringIndexer(inputCol=columnName, outputCol=columnName + \"idx\", handleInvalid=\"keep\") for columnName in textFeatures]\n",
    "#Encoder maps integer to vector\n",
    "encoder = [OneHotEncoder(inputCol=columnName + \"idx\", outputCol=columnName + \"vector\") for columnName in textFeatures]\n",
    "\n",
    "assembler = VectorAssembler(inputCols= numFeatures + boolFeatures + [columnName + \"vector\" for columnName in textFeatures], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol= \"features\", outputCol=\"features-Scaled\")\n",
    "\n",
    "potentialModels = {\"LinearRegression\": LinearRegression(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"GBT Regressor\": GBTRegressor(featuresCol=\"features-Scaled\", labelCol=target),\n",
    "                   \"Random Forest\" : RandomForestRegressor(featuresCol=\"features-Scaled\", labelCol=target)\n",
    "                   }\n",
    "\n",
    "evalObj = RegressionEvaluator(metricName=\"rmse\", labelCol=target, predictionCol=\"prediction\")\n",
    "\n",
    "colsToDrop = numFeatures + boolFeatures + textFeatures + [columnName + \"idx\" for columnName in textFeatures] + [columnName + \"vector\" for columnName in textFeatures]\n",
    "\n",
    "\n",
    "#for modelName, model in potentialModels.items():\n",
    "#   print(f\"Commencing evaluation on {modelName} !\")\n",
    "#   pipeline = Pipeline(stages = indexer + encoder + [assembler, scaler, model])\n",
    "#   fullPipelineObj = pipeline.fit(trainDf)\n",
    "#\n",
    "#   predictions = fullPipelineObj.transform(testDf)\n",
    "    \n",
    "#   predictions.drop(*colsToDrop)\n",
    "    \n",
    "#   rmse = evalObj.evaluate(predictions)\n",
    "\n",
    "#   print(f\"{modelName} = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fbc5a",
   "metadata": {},
   "source": [
    "Commencing evaluation on LinearRegression !\n",
    "LinearRegression = 1.4334905085709757\n",
    "Commencing evaluation on GBT Regressor !\n",
    "GBT Regressor = 1.4419406537003188\n",
    "Commencing evaluation on Random Forest !\n",
    "Random Forest = 1.4300720209132907\n",
    "\n",
    "for 90% sampling \n",
    "Commencing evaluation on LinearRegression !\n",
    "LinearRegression = 1.433266075915844\n",
    "Commencing evaluation on GBT Regressor !\n",
    "GBT Regressor = 1.442627337619911\n",
    "Commencing evaluation on Random Forest !\n",
    "Random Forest = 1.431955721144724\n",
    "\n",
    "for 70% sampling \n",
    "Commencing evaluation on LinearRegression !\n",
    "LinearRegression = 1.5311121164404962\n",
    "Commencing evaluation on GBT Regressor !\n",
    "GBT Regressor = 1.5562678111012818\n",
    "Commencing evaluation on Random Forest !\n",
    "Random Forest = 1.5313389438322291\n",
    "\n",
    "for 50% Sampling \n",
    "Commencing evaluation on LinearRegression !\n",
    "LinearRegression = 1.6648558482495757\n",
    "Commencing evaluation on GBT Regressor !\n",
    "GBT Regressor = 1.685530584445583\n",
    "Commencing evaluation on Random Forest !\n",
    "Random Forest = 1.6646815092344738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2cace",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o726.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9) (DESKTOP-DEBTK35 executor driver): java.io.IOException: There is not enough space on the disk\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:284)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:232)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.baseCacheRDD(InMemoryTableScanExec.scala:202)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.future$lzycompute(QueryStageExec.scala:296)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.future(QueryStageExec.scala:292)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.doMaterialize(QueryStageExec.scala:307)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:71)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$5(AdaptiveSparkPlanExec.scala:309)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$5$adapted(AdaptiveSparkPlanExec.scala:307)\r\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:307)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collect$1(Dataset.scala:1482)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.collect(Dataset.scala:1482)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:213)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:152)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: There is not enough space on the disk\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#2 folds of cross validation, evalobj = rmse\u001b[39;00m\n\u001b[32m     16\u001b[39m CrossVal = CrossValidator(\n\u001b[32m     17\u001b[39m     estimator=pipeline,\n\u001b[32m     18\u001b[39m     estimatorParamMaps=paraGrid,\n\u001b[32m     19\u001b[39m     evaluator=evalObj,\n\u001b[32m     20\u001b[39m     numFolds=\u001b[32m2\u001b[39m,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m CrossValModel = \u001b[43mCrossVal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDf\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     25\u001b[39m predictions = CrossValModel.transform(testDf)\n\u001b[32m     26\u001b[39m rmse = evalObj.evaluate(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    852\u001b[39m train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit.<locals>.<lambda>\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m    852\u001b[39m train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool.imap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[32m    859\u001b[39m     metrics_all[i][j] = metric\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\util.py:435\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[32m    434\u001b[39m     session.addTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\tuning.py:115\u001b[39m, in \u001b[36m_parallelFitTasks.<locals>.singleTask\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingleTask\u001b[39m() -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     index, model = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[32m    120\u001b[39m     metric = eva.evaluate(model.transform(validation, epm[index]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:96\u001b[39m, in \u001b[36m_FitMultipleIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo models remaining.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.counter += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:154\u001b[39m, in \u001b[36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) -> M:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:201\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\pipeline.py:138\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    136\u001b[39m     dataset = stage.transform(dataset)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     transformers.append(model)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < indexOfLastEstimator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o726.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9) (DESKTOP-DEBTK35 executor driver): java.io.IOException: There is not enough space on the disk\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:284)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:232)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.baseCacheRDD(InMemoryTableScanExec.scala:202)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.future$lzycompute(QueryStageExec.scala:296)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.future(QueryStageExec.scala:292)\r\n\tat org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec.doMaterialize(QueryStageExec.scala:307)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:71)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$5(AdaptiveSparkPlanExec.scala:309)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$5$adapted(AdaptiveSparkPlanExec.scala:307)\r\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:307)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:279)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collect$1(Dataset.scala:1482)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.collect(Dataset.scala:1482)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:213)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:152)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: There is not enough space on the disk\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "#Selected Model\n",
    "RFR = RandomForestRegressor(featuresCol=\"features-Scaled\", labelCol=target)\n",
    "\n",
    "#Define pipeline\n",
    "pipeline = Pipeline(stages = indexer + encoder + [assembler, scaler, RFR])\n",
    "\n",
    "paraGrid = ParamGridBuilder() \\\n",
    "    .addGrid(RFR.numTrees, [20, 50, 100]) \\\n",
    "    .addGrid(RFR.maxDepth, [5, 10]) \\\n",
    "    .addGrid(RFR.minInstancesPerNode, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "#2 folds of cross validation, evalobj = rmse\n",
    "CrossVal = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paraGrid,\n",
    "    evaluator=evalObj,\n",
    "    numFolds=2,\n",
    ")\n",
    "\n",
    "CrossValModel = CrossVal.fit(trainDf) \n",
    "\n",
    "predictions = CrossValModel.transform(testDf)\n",
    "rmse = evalObj.evaluate(predictions)\n",
    "print(f\"Optimised RMSE: {rmse}\")\n",
    "\n",
    "#To save memory\n",
    "trainDf.unpersist()\n",
    "del trainDf\n",
    "testDf.unpersist()\n",
    "del testDf\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "bestModel = CrossValModel.bestModel\n",
    "\n",
    "RFR_model = bestModel.stages[-1]\n",
    "\n",
    "bestPara = {\n",
    "    'numTrees': RFR_model.getNumTrees,\n",
    "    'maxDepth': RFR_model.getMaxDepth(),\n",
    "    'minInstancesPerNode': RFR_model.getMinInstancesPerNode(),\n",
    "    'featureSubsetStrategy': RFR_model.getFeatureSubsetStrategy()\n",
    "}\n",
    "\n",
    "for hyperPara, value in bestPara.items():\n",
    "    print(f\"  {hyperPara}: {value}\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
